{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.io as sio\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from neuralop.models import FNO\n",
    "from neuralop import Trainer\n",
    "from neuralop.training import AdamW\n",
    "from neuralop import LpLoss, H1Loss\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop.data.transforms.normalizers import UnitGaussianNormalizer\n",
    "from abc import abstractmethod\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 data files.\n",
      "Loading training data into memory...\n",
      "Full training sequence shape: torch.Size([20200, 3, 32, 32, 32])\n",
      "Loading test data into memory...\n",
      "Full test sequence shape: torch.Size([80800, 3, 32, 32, 32])\n",
      "Fitting normalizer on training data...\n",
      "Fit complete.\n",
      "Normalizing data...\n",
      "Normalized train data mean: -2.0463277738969055e-09\n",
      "Normalized train data std: 0.9999977946281433\n",
      "Normalization complete. Raw data cleared from RAM.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Find and Load All Data Files ---\n",
    "data_path = './FNO_Dataset_PT/'\n",
    "file_paths = glob.glob(f\"{data_path}/FNO_dataset_run_*.pt\")\n",
    "file_paths.sort()\n",
    "\n",
    "if not file_paths:\n",
    "    raise FileNotFoundError(f\"No .mat files found in {data_path}\")\n",
    "\n",
    "print(f\"Found {len(file_paths)} data files.\")\n",
    "\n",
    "# --- 2. Split File Paths into Train and Test ---\n",
    "train_split = int(0.2 * len(file_paths))\n",
    "train_paths = file_paths[:train_split]\n",
    "test_paths = file_paths[train_split:]\n",
    "\n",
    "# --- 3. Load ALL Data into RAM for Normalization ---\n",
    "# This is the workflow you prefer. It requires\n",
    "# loading all training data into memory first.\n",
    "\n",
    "def load_data_from_paths(paths): # <-- Removed data_key\n",
    "    all_tensors = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            # Load the .pt file directly as a tensor\n",
    "            tensor_data = torch.load(p).float() # <-- Changed loading function\n",
    "            all_tensors.append(tensor_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error loading {p}: {e}\")\n",
    "    # Concatenate all runs along the time dimension (dim=0)\n",
    "    return torch.cat(all_tensors, dim=0)\n",
    "\n",
    "print(\"Loading training data into memory...\")\n",
    "train_data_sequence = load_data_from_paths(train_paths)\n",
    "print(f\"Full training sequence shape: {train_data_sequence.shape}\")\n",
    "\n",
    "print(\"Loading test data into memory...\")\n",
    "test_data_sequence = load_data_from_paths(test_paths)\n",
    "print(f\"Full test sequence shape: {test_data_sequence.shape}\")\n",
    "\n",
    "# --- 4. Fit and Transform (Your Method) ---\n",
    "# Create the normalizer\n",
    "normalizer = UnitGaussianNormalizer(dim=[0, 2, 3, 4]) \n",
    "\n",
    "# Fit ONLY on the training data\n",
    "print(\"Fitting normalizer on training data...\")\n",
    "normalizer.fit(train_data_sequence)\n",
    "print(\"Fit complete.\")\n",
    "\n",
    "# Transform both sets\n",
    "print(\"Normalizing data...\")\n",
    "train_data = normalizer.transform(train_data_sequence)\n",
    "test_data = normalizer.transform(test_data_sequence)\n",
    "\n",
    "# --- ADD THIS SANITY CHECK ---\n",
    "print(f\"Normalized train data mean: {train_data.mean()}\")\n",
    "print(f\"Normalized train data std: {train_data.std()}\")\n",
    "# -----------------------------\n",
    "\n",
    "# Free up memory\n",
    "del train_data_sequence\n",
    "del test_data_sequence\n",
    "print(\"Normalization complete. Raw data cleared from RAM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Define Simple Dataset Class ---\n",
    "class TimeSteppingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple dataset that just returns the (t, t+1) pairs\n",
    "    from a pre-normalized data sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_sequence):\n",
    "        self.data = data_sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'x': self.data[idx], 'y': self.data[idx + 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Create Datasets and DataLoaders ---\n",
    "# Create the datasets from your NEW normalized tensors\n",
    "train_dataset = TimeSteppingDataset(train_data)\n",
    "test_dataset = TimeSteppingDataset(test_data)\n",
    "\n",
    "# Create the DataLoaders\n",
    "# Try a small batch size first due to memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 590579 parameters.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Define Model, Optimizer, Loss ---\n",
    "model = FNO(\n",
    "    n_modes=(16, 16, 16),\n",
    "    hidden_channels=8,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    n_layers=2\n",
    ").to(device) \n",
    "\n",
    "print(f\"Model has {count_model_params(model)} parameters.\")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4) # Using the lower 1e-4 lr\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "l2loss = LpLoss(d=3, p=2)\n",
    "h1loss = H1Loss(d=3)\n",
    "\n",
    "# --- 8. Create Trainer (No Processor) ---\n",
    "trainer = Trainer(model=model, n_epochs=1000,\n",
    "                  device=device,\n",
    "                  wandb_log=False,\n",
    "                  eval_interval=1,\n",
    "                  use_distributed=False,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on full, normalized dataset...\n",
      "Training on 20199 samples\n",
      "Testing on [80799] samples         on resolutions [32].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prad/.conda/envs/fno_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1786: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw outputs of shape torch.Size([100, 3, 32, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prad/personal/CS6886/cs6886-jul-nov-2025-course-project-rephino/sloshing_neural_operator/neuraloperator/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.\n",
      "  loss += training_loss(out, **sample)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] time=32.95, avg_loss=847389368.4559, train_err=84734741848.7129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prad/personal/CS6886/cs6886-jul-nov-2025-course-project-rephino/sloshing_neural_operator/neuraloperator/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.\n",
      "  val_loss = loss(out, **sample)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: 32_h1=301908352.0000, 32_l2=224846240.0000\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[1] time=30.92, avg_loss=101753114.4251, train_err=10174807714.2178\n",
      "Eval: 32_h1=44625072.0000, 32_l2=4055993.7500\n",
      "[2] time=32.18, avg_loss=31942527.1160, train_err=3194094580.2772\n",
      "Eval: 32_h1=22646602.0000, 32_l2=2233443.7500\n",
      "[3] time=32.09, avg_loss=17798774.0256, train_err=1779789289.8218\n",
      "Eval: 32_h1=14318814.0000, 32_l2=1452944.5000\n",
      "[4] time=31.30, avg_loss=11858983.9790, train_err=1185839690.0594\n",
      "Eval: 32_h1=10272504.0000, 32_l2=1047596.0000\n",
      "[5] time=31.33, avg_loss=8838845.5048, train_err=883840793.8218\n",
      "Eval: 32_h1=8060312.0000, 32_l2=813801.0000\n",
      "[6] time=32.46, avg_loss=7091570.3375, train_err=709121926.9703\n",
      "Eval: 32_h1=6752081.5000, 32_l2=939951.3750\n",
      "[7] time=32.99, avg_loss=5985795.7812, train_err=598549945.4653\n",
      "Eval: 32_h1=5791120.0000, 32_l2=729962.0625\n",
      "[8] time=32.01, avg_loss=5240282.2409, train_err=524002282.0990\n",
      "Eval: 32_h1=5051724.0000, 32_l2=530373.5000\n",
      "[9] time=31.22, avg_loss=4772847.7748, train_err=477261149.5248\n",
      "Eval: 32_h1=4557417.5000, 32_l2=461205.9375\n",
      "[10] time=32.17, avg_loss=4303178.6284, train_err=430296559.9802\n",
      "Eval: 32_h1=4212123.0000, 32_l2=651150.3125\n",
      "[11] time=31.61, avg_loss=3972380.5533, train_err=397218390.0792\n",
      "Eval: 32_h1=3776322.7500, 32_l2=344477.9688\n",
      "[12] time=31.25, avg_loss=3652267.8580, train_err=365208705.2673\n",
      "Eval: 32_h1=3502199.7500, 32_l2=376612.3438\n",
      "[13] time=31.33, avg_loss=3361378.8746, train_err=336121246.9703\n",
      "Eval: 32_h1=3496375.2500, 32_l2=520940.3125\n",
      "[14] time=32.38, avg_loss=3142335.5270, train_err=314217996.5842\n",
      "Eval: 32_h1=3714831.7500, 32_l2=1033165.6250\n",
      "[15] time=35.88, avg_loss=2975598.9757, train_err=297545166.8812\n",
      "Eval: 32_h1=2841427.5000, 32_l2=269639.5625\n",
      "[16] time=33.97, avg_loss=2813319.8523, train_err=281318057.9010\n",
      "Eval: 32_h1=3087130.5000, 32_l2=780884.8125\n",
      "[17] time=32.47, avg_loss=2613958.7115, train_err=261382930.7624\n",
      "Eval: 32_h1=2559462.2500, 32_l2=309950.7188\n",
      "[18] time=34.24, avg_loss=2584932.4153, train_err=258480444.8317\n",
      "Eval: 32_h1=2491512.5000, 32_l2=374551.3750\n",
      "[19] time=32.49, avg_loss=2327854.6247, train_err=232773938.4356\n",
      "Eval: 32_h1=2329115.0000, 32_l2=282844.8750\n",
      "[20] time=32.54, avg_loss=2215860.2184, train_err=221575052.2376\n",
      "Eval: 32_h1=2387510.2500, 32_l2=462987.8125\n",
      "[21] time=31.65, avg_loss=2229119.3676, train_err=222900901.5149\n",
      "Eval: 32_h1=2478179.5000, 32_l2=708855.0625\n",
      "[22] time=32.63, avg_loss=2099614.8771, train_err=209951093.5743\n",
      "Eval: 32_h1=2503245.2500, 32_l2=706357.5000\n",
      "[23] time=34.02, avg_loss=2009270.5209, train_err=200917105.2079\n",
      "Eval: 32_h1=2080418.6250, 32_l2=422064.5312\n",
      "[24] time=35.65, avg_loss=1827609.6963, train_err=182751922.0545\n",
      "Eval: 32_h1=1927040.2500, 32_l2=269509.0312\n",
      "[25] time=30.72, avg_loss=1770771.7012, train_err=177068403.9257\n",
      "Eval: 32_h1=1991543.2500, 32_l2=452717.6250\n",
      "[26] time=33.63, avg_loss=1690321.3219, train_err=169023764.2624\n",
      "Eval: 32_h1=1758926.7500, 32_l2=219417.9844\n",
      "[27] time=32.33, avg_loss=1613847.1634, train_err=161376726.9950\n",
      "Eval: 32_h1=1683884.7500, 32_l2=148390.2344\n",
      "[28] time=31.46, avg_loss=1639492.7388, train_err=163941157.5792\n",
      "Eval: 32_h1=1640617.1250, 32_l2=157299.7500\n",
      "[29] time=31.65, avg_loss=1538243.0305, train_err=153816687.9851\n",
      "Eval: 32_h1=1624875.3750, 32_l2=233906.9062\n",
      "[30] time=32.04, avg_loss=1477013.8046, train_err=147694068.5099\n",
      "Eval: 32_h1=1553607.5000, 32_l2=158740.8438\n",
      "[31] time=31.18, avg_loss=1437900.9735, train_err=143782979.0248\n",
      "Eval: 32_h1=1530043.1250, 32_l2=189263.4844\n",
      "[32] time=31.16, avg_loss=1400005.7502, train_err=139993644.2970\n",
      "Eval: 32_h1=1477239.8750, 32_l2=125172.8438\n",
      "[33] time=31.83, avg_loss=1388275.9198, train_err=138820719.3267\n",
      "Eval: 32_h1=1494207.2500, 32_l2=220779.9844\n",
      "[34] time=31.39, avg_loss=1354074.9322, train_err=135400789.8762\n",
      "Eval: 32_h1=1418481.1250, 32_l2=113327.4453\n",
      "[35] time=31.31, avg_loss=1315679.3385, train_err=131561420.5842\n",
      "Eval: 32_h1=1395334.8750, 32_l2=114782.2812\n",
      "[36] time=31.83, avg_loss=1294211.0221, train_err=129414695.2277\n",
      "Eval: 32_h1=1373093.5000, 32_l2=109495.5859\n",
      "[37] time=32.04, avg_loss=1274714.6518, train_err=127465154.7129\n",
      "Eval: 32_h1=1353785.1250, 32_l2=107506.4844\n",
      "[38] time=32.26, avg_loss=1257406.2891, train_err=125734404.1287\n",
      "Eval: 32_h1=1336740.7500, 32_l2=107841.0312\n",
      "[39] time=31.34, avg_loss=1242336.7502, train_err=124227524.8416\n",
      "Eval: 32_h1=1322835.0000, 32_l2=106627.4766\n",
      "[40] time=31.40, avg_loss=1229485.4507, train_err=122942458.5099\n",
      "Eval: 32_h1=1309972.2500, 32_l2=107091.1719\n",
      "[41] time=31.69, avg_loss=1218377.6425, train_err=121831732.6733\n",
      "Eval: 32_h1=1299646.3750, 32_l2=102604.3984\n",
      "[42] time=31.70, avg_loss=1209439.5318, train_err=120937965.8515\n",
      "Eval: 32_h1=1290739.7500, 32_l2=102096.3984\n",
      "[43] time=31.10, avg_loss=1202004.2402, train_err=120194473.5050\n",
      "Eval: 32_h1=1283798.5000, 32_l2=101042.5078\n",
      "[44] time=31.72, avg_loss=1196161.1845, train_err=119610196.8614\n",
      "Eval: 32_h1=1278761.2500, 32_l2=100524.0547\n",
      "[45] time=32.16, avg_loss=1191870.1493, train_err=119181114.5842\n",
      "Eval: 32_h1=1275140.0000, 32_l2=100254.5469\n",
      "[46] time=31.01, avg_loss=1188804.3269, train_err=118874547.5248\n",
      "Eval: 32_h1=1272614.3750, 32_l2=100096.4375\n",
      "[47] time=31.48, avg_loss=1186836.6933, train_err=118677793.9010\n",
      "Eval: 32_h1=1271055.0000, 32_l2=99889.5391\n",
      "[48] time=31.89, avg_loss=1185710.3539, train_err=118565165.5347\n",
      "Eval: 32_h1=1270321.3750, 32_l2=99806.2266\n",
      "[49] time=31.58, avg_loss=1185265.4049, train_err=118520672.8416\n",
      "Eval: 32_h1=1270160.6250, 32_l2=99777.6016\n",
      "[50] time=31.21, avg_loss=1185176.0772, train_err=118511740.5099\n",
      "Eval: 32_h1=1270160.6250, 32_l2=99777.6016\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[51] time=31.82, avg_loss=1185091.4077, train_err=118503273.9802\n",
      "Eval: 32_h1=1269965.0000, 32_l2=99763.7031\n",
      "[52] time=31.63, avg_loss=1184665.1416, train_err=118460649.4802\n",
      "Eval: 32_h1=1269121.3750, 32_l2=99696.0547\n",
      "[53] time=32.28, avg_loss=1183444.3331, train_err=118338574.6733\n",
      "Eval: 32_h1=1267197.2500, 32_l2=99564.2891\n",
      "[54] time=31.45, avg_loss=1180963.4924, train_err=118090502.8812\n",
      "Eval: 32_h1=1263675.6250, 32_l2=99362.5078\n",
      "[55] time=31.73, avg_loss=1176863.3702, train_err=117680510.9653\n",
      "Eval: 32_h1=1258048.3750, 32_l2=99117.1406\n",
      "[56] time=32.28, avg_loss=1172248.3996, train_err=117219036.7525\n",
      "Eval: 32_h1=1251309.8750, 32_l2=99970.1641\n",
      "[57] time=31.36, avg_loss=1169733.2728, train_err=116967536.5198\n",
      "Eval: 32_h1=1245297.3750, 32_l2=121493.3828\n",
      "[58] time=31.24, avg_loss=1154574.5936, train_err=115451743.6436\n",
      "Eval: 32_h1=1232047.3750, 32_l2=98818.5938\n",
      "[59] time=31.89, avg_loss=1145405.6231, train_err=114534891.9802\n",
      "Eval: 32_h1=1220969.8750, 32_l2=102050.3828\n",
      "[60] time=31.37, avg_loss=1141027.8428, train_err=114097135.6238\n",
      "Eval: 32_h1=1206798.0000, 32_l2=98552.3750\n",
      "[61] time=31.34, avg_loss=1128489.8115, train_err=112843394.5693\n",
      "Eval: 32_h1=1193669.0000, 32_l2=112263.7031\n",
      "[62] time=32.10, avg_loss=1125048.2373, train_err=112499254.1832\n",
      "Eval: 32_h1=1176769.5000, 32_l2=96025.0859\n",
      "[63] time=31.90, avg_loss=1111894.8579, train_err=111183981.3614\n",
      "Eval: 32_h1=1160655.7500, 32_l2=95493.2109\n",
      "[64] time=31.63, avg_loss=1107133.5088, train_err=110707870.0248\n",
      "Eval: 32_h1=1143396.8750, 32_l2=98490.4219\n",
      "[65] time=31.58, avg_loss=1080047.5066, train_err=107999403.8911\n",
      "Eval: 32_h1=1259599.2500, 32_l2=301243.7500\n",
      "[66] time=31.97, avg_loss=1083529.4459, train_err=108347580.5792\n",
      "Eval: 32_h1=1130614.5000, 32_l2=149672.3906\n",
      "[67] time=31.69, avg_loss=1082199.5285, train_err=108214595.4307\n",
      "Eval: 32_h1=1126094.8750, 32_l2=188919.2500\n",
      "[68] time=30.96, avg_loss=1086423.5824, train_err=108636979.9010\n",
      "Eval: 32_h1=1145751.8750, 32_l2=247267.1562\n",
      "[69] time=31.42, avg_loss=1050101.4727, train_err=105004948.7475\n",
      "Eval: 32_h1=1085694.5000, 32_l2=199186.8594\n",
      "[70] time=31.52, avg_loss=1210842.5419, train_err=121078259.9158\n",
      "Eval: 32_h1=1148752.5000, 32_l2=272284.0000\n",
      "[71] time=31.32, avg_loss=1018776.4215, train_err=101872598.6980\n",
      "Eval: 32_h1=992970.1250, 32_l2=90789.1328\n",
      "[72] time=31.32, avg_loss=995612.5135, train_err=99556322.5743\n",
      "Eval: 32_h1=1051757.3750, 32_l2=224149.3281\n",
      "[73] time=31.87, avg_loss=1135516.0167, train_err=113545980.3020\n",
      "Eval: 32_h1=1078253.2500, 32_l2=270291.9375\n",
      "[74] time=31.06, avg_loss=1006757.9527, train_err=100670811.3168\n",
      "Eval: 32_h1=986688.6875, 32_l2=206408.0938\n",
      "[75] time=31.19, avg_loss=996639.8284, train_err=99659048.9802\n",
      "Eval: 32_h1=1100500.0000, 32_l2=378060.7812\n",
      "[76] time=31.85, avg_loss=987126.2494, train_err=98707738.1733\n",
      "Eval: 32_h1=908375.8125, 32_l2=177525.5625\n",
      "[77] time=31.27, avg_loss=966204.0284, train_err=96615619.6485\n",
      "Eval: 32_h1=911405.6875, 32_l2=208001.5781\n",
      "[78] time=30.90, avg_loss=981488.6815, train_err=98144009.2921\n",
      "Eval: 32_h1=836820.5000, 32_l2=122436.2656\n",
      "[79] time=31.56, avg_loss=1008006.9297, train_err=100795702.8416\n",
      "Eval: 32_h1=928396.0000, 32_l2=256940.7344\n",
      "[80] time=31.95, avg_loss=974810.2938, train_err=97476203.5891\n",
      "Eval: 32_h1=843866.3750, 32_l2=152219.9062\n",
      "[81] time=31.17, avg_loss=946562.4862, train_err=94651562.6683\n",
      "Eval: 32_h1=1137465.7500, 32_l2=546471.0000\n",
      "[82] time=31.67, avg_loss=1064772.1676, train_err=106471945.6139\n",
      "Eval: 32_h1=803750.1875, 32_l2=88790.2188\n",
      "[83] time=31.71, avg_loss=1030652.0144, train_err=103060099.2030\n",
      "Eval: 32_h1=876493.3125, 32_l2=256593.8281\n",
      "[84] time=31.83, avg_loss=938882.4906, train_err=93883601.1238\n",
      "Eval: 32_h1=777910.5625, 32_l2=199074.5156\n",
      "[85] time=30.89, avg_loss=1077483.2840, train_err=107742994.3267\n",
      "Eval: 32_h1=1006956.9375, 32_l2=185542.6562\n",
      "[86] time=31.95, avg_loss=941378.6494, train_err=94133204.6535\n",
      "Eval: 32_h1=726852.0000, 32_l2=200014.9844\n",
      "[87] time=32.00, avg_loss=910117.9387, train_err=91007288.3366\n",
      "Eval: 32_h1=877414.9375, 32_l2=316676.6250\n",
      "[88] time=32.17, avg_loss=908949.5195, train_err=90890452.1980\n",
      "Eval: 32_h1=742256.3750, 32_l2=242385.3750\n",
      "[89] time=31.07, avg_loss=1065529.9673, train_err=106547721.8317\n",
      "Eval: 32_h1=707055.0625, 32_l2=218210.6719\n",
      "[90] time=31.46, avg_loss=942591.0893, train_err=94254442.6337\n",
      "Eval: 32_h1=851886.4375, 32_l2=280810.5000\n",
      "[91] time=32.16, avg_loss=990785.2831, train_err=99073623.4356\n",
      "Eval: 32_h1=870901.6875, 32_l2=413872.8438\n",
      "[92] time=31.61, avg_loss=870374.4367, train_err=87033134.8861\n",
      "Eval: 32_h1=925908.9375, 32_l2=512169.6250\n",
      "[93] time=30.95, avg_loss=866031.3308, train_err=86598845.7921\n",
      "Eval: 32_h1=657158.5000, 32_l2=237623.0312\n",
      "[94] time=32.02, avg_loss=978036.0801, train_err=97798766.2426\n",
      "Eval: 32_h1=1252996.7500, 32_l2=839925.2500\n",
      "[95] time=31.33, avg_loss=1080520.3911, train_err=108046690.0000\n",
      "Eval: 32_h1=1253746.6250, 32_l2=860262.7500\n",
      "[96] time=31.23, avg_loss=947752.9975, train_err=94770607.9059\n",
      "Eval: 32_h1=622128.5000, 32_l2=274019.7812\n",
      "[97] time=31.47, avg_loss=903833.4103, train_err=90378866.6089\n",
      "Eval: 32_h1=1980342.8750, 32_l2=1655219.8750\n",
      "[98] time=31.75, avg_loss=967843.1033, train_err=96779519.0297\n",
      "Eval: 32_h1=686453.8125, 32_l2=319150.3750\n",
      "[99] time=31.75, avg_loss=838096.5841, train_err=83805509.4158\n",
      "Eval: 32_h1=545018.2500, 32_l2=173547.3594\n",
      "[100] time=31.64, avg_loss=814390.0286, train_err=81434971.2277\n",
      "Eval: 32_h1=1139605.3750, 32_l2=836783.5625\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[101] time=31.08, avg_loss=850155.2332, train_err=85011314.6287\n",
      "Eval: 32_h1=832751.7500, 32_l2=557001.1875\n",
      "[102] time=31.81, avg_loss=807597.8134, train_err=80755783.3267\n",
      "Eval: 32_h1=1460285.3750, 32_l2=984400.2500\n",
      "[103] time=31.56, avg_loss=957045.2831, train_err=95699790.4653\n",
      "Eval: 32_h1=754954.1875, 32_l2=489141.9688\n",
      "[104] time=31.06, avg_loss=868446.6863, train_err=86840369.3861\n",
      "Eval: 32_h1=647195.1875, 32_l2=357146.7500\n",
      "[105] time=31.45, avg_loss=786200.3737, train_err=78616145.2921\n",
      "Eval: 32_h1=809427.0625, 32_l2=579079.0625\n",
      "[106] time=32.19, avg_loss=740262.1521, train_err=74022550.5446\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Use the shape of one test sample as the key\u001b[39;00m\n\u001b[32m      4\u001b[39m test_key = test_data[\u001b[32m0\u001b[39m].shape[\u001b[32m1\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m              \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43mtest_key\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m              \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m              \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m              \u001b[49m\u001b[43mtraining_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mh1loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m              \u001b[49m\u001b[43meval_losses\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mh1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mh1loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ml2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2loss\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                \u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./checkpoints/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal/CS6886/cs6886-jul-nov-2025-course-project-rephino/sloshing_neural_operator/neuraloperator/neuralop/training/trainer.py:245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_loader, test_loaders, optimizer, scheduler, regularizer, training_loss, eval_losses, eval_modes, save_every, save_best, save_dir, resume_from_dir, max_autoregressive_steps)\u001b[39m\n\u001b[32m    236\u001b[39m epoch_metrics = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    237\u001b[39m     train_err=train_err,\n\u001b[32m    238\u001b[39m     avg_loss=avg_loss,\n\u001b[32m    239\u001b[39m     avg_lasso_loss=avg_lasso_loss,\n\u001b[32m    240\u001b[39m     epoch_train_time=epoch_train_time,\n\u001b[32m    241\u001b[39m )\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[38;5;28mself\u001b[39m.eval_interval == \u001b[32m0\u001b[39m:\n\u001b[32m    244\u001b[39m     \u001b[38;5;66;03m# evaluate and gather metrics across each loader in test_loaders\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     eval_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_all\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_losses\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_losses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_modes\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_modes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_autoregressive_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_autoregressive_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m     epoch_metrics.update(**eval_metrics)\n\u001b[32m    253\u001b[39m     \u001b[38;5;66;03m# save checkpoint if conditions are met\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal/CS6886/cs6886-jul-nov-2025-course-project-rephino/sloshing_neural_operator/neuraloperator/neuralop/training/trainer.py:373\u001b[39m, in \u001b[36mTrainer.evaluate_all\u001b[39m\u001b[34m(self, epoch, eval_losses, test_loaders, eval_modes, max_autoregressive_steps)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m loader_name, loader \u001b[38;5;129;01min\u001b[39;00m test_loaders.items():\n\u001b[32m    372\u001b[39m     loader_eval_mode = eval_modes.get(loader_name, \u001b[33m\"\u001b[39m\u001b[33msingle_step\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     loader_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_losses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloader_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloader_eval_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_autoregressive_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m     all_metrics.update(**loader_metrics)\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal/CS6886/cs6886-jul-nov-2025-course-project-rephino/sloshing_neural_operator/neuraloperator/neuralop/training/trainer.py:442\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, loss_dict, data_loader, log_prefix, epoch, mode, max_steps)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.n_samples = \u001b[32m0\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_output\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:171\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections.abc.MutableMapping):\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[32m    169\u001b[39m     clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m     clone.update(\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[43m{\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:172\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections.abc.MutableMapping):\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[32m    169\u001b[39m     clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m     clone.update(\n\u001b[32m    171\u001b[39m         {\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[32m    176\u001b[39m         }\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- 9. Start Training ---\n",
    "print(\"Starting training on full, normalized dataset...\")\n",
    "# Use the shape of one test sample as the key\n",
    "test_key = test_data[0].shape[1]\n",
    "trainer.train(train_loader=train_loader,\n",
    "              test_loaders={test_key: test_loader},\n",
    "              optimizer=optimizer,\n",
    "              scheduler=scheduler,\n",
    "              training_loss=h1loss,\n",
    "              eval_losses={'h1': h1loss, 'l2': l2loss},\n",
    "                save_every=50,\n",
    "                save_dir='./checkpoints/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fno_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
