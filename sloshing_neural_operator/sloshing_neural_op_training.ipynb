{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prad/miniconda3/envs/torch_env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/prad/miniconda3/envs/torch_env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy.io as sio\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from neuralop.models import FNO\n",
    "from neuralop import Trainer\n",
    "from neuralop.training import AdamW\n",
    "from neuralop import LpLoss, H1Loss\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop.data.transforms.normalizers import UnitGaussianNormalizer\n",
    "from abc import abstractmethod\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 data files.\n",
      "Loading training data into memory...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(all_tensors, dim=\u001b[32m0\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading training data into memory...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m train_data_sequence = \u001b[43mload_data_from_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFull training sequence shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_data_sequence.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading test data into memory...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mload_data_from_paths\u001b[39m\u001b[34m(paths, data_key)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         mat_data = \u001b[43msio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m         tensor_data = torch.tensor(mat_data[data_key]).float()\n\u001b[32m     26\u001b[39m         all_tensors.append(tensor_data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/scipy/io/matlab/_mio.py:235\u001b[39m, in \u001b[36mloadmat\u001b[39m\u001b[34m(file_name, mdict, appendmat, spmatrix, **kwargs)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    234\u001b[39m     MR, _ = mat_reader_factory(f, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     matfile_dict = \u001b[43mMR\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spmatrix:\n\u001b[32m    237\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m issparse, coo_matrix\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/scipy/io/matlab/_mio5.py:333\u001b[39m, in \u001b[36mMatFile5Reader.get_variables\u001b[39m\u001b[34m(self, variable_names)\u001b[39m\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_var_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MatReadError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    335\u001b[39m     warnings.warn(\n\u001b[32m    336\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnreadable variable \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m, because \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    337\u001b[39m         \u001b[38;5;167;01mWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/scipy/io/matlab/_mio5.py:291\u001b[39m, in \u001b[36mMatFile5Reader.read_var_array\u001b[39m\u001b[34m(self, header, process)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_var_array\u001b[39m(\u001b[38;5;28mself\u001b[39m, header, process=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    275\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m''' Read array, given `header`\u001b[39;00m\n\u001b[32m    276\u001b[39m \n\u001b[32m    277\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    289\u001b[39m \u001b[33;03m       `process`.\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_matrix_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_from_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- 1. Find and Load All Data Files ---\n",
    "data_path = './FNO_Dataset/'\n",
    "file_paths = glob.glob(f\"{data_path}/FNO_dataset_run_*.mat\")\n",
    "file_paths.sort()\n",
    "\n",
    "if not file_paths:\n",
    "    raise FileNotFoundError(f\"No .mat files found in {data_path}\")\n",
    "\n",
    "print(f\"Found {len(file_paths)} data files.\")\n",
    "\n",
    "# --- 2. Split File Paths into Train and Test ---\n",
    "train_split = int(0.8 * len(file_paths))\n",
    "train_paths = file_paths[:train_split]\n",
    "test_paths = file_paths[train_split:]\n",
    "\n",
    "# --- 3. Load ALL Data into RAM for Normalization ---\n",
    "# This is the workflow you prefer. It requires\n",
    "# loading all training data into memory first.\n",
    "\n",
    "def load_data_from_paths(paths, data_key='velocity_field_5D'):\n",
    "    all_tensors = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            mat_data = sio.loadmat(p)\n",
    "            tensor_data = torch.tensor(mat_data[data_key]).float()\n",
    "            all_tensors.append(tensor_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error loading {p}: {e}\")\n",
    "    # Concatenate all runs along the time dimension (dim=0)\n",
    "    return torch.cat(all_tensors, dim=0)\n",
    "\n",
    "print(\"Loading training data into memory...\")\n",
    "train_data_sequence = load_data_from_paths(train_paths)\n",
    "print(f\"Full training sequence shape: {train_data_sequence.shape}\")\n",
    "\n",
    "print(\"Loading test data into memory...\")\n",
    "test_data_sequence = load_data_from_paths(test_paths)\n",
    "print(f\"Full test sequence shape: {test_data_sequence.shape}\")\n",
    "\n",
    "# --- 4. Fit and Transform (Your Method) ---\n",
    "# Create the normalizer\n",
    "normalizer = UnitGaussianNormalizer(dim=[0, 2, 3, 4]) \n",
    "\n",
    "# Fit ONLY on the training data\n",
    "print(\"Fitting normalizer on training data...\")\n",
    "normalizer.fit(train_data_sequence)\n",
    "print(\"Fit complete.\")\n",
    "\n",
    "# Transform both sets\n",
    "print(\"Normalizing data...\")\n",
    "train_data = normalizer.transform(train_data_sequence)\n",
    "test_data = normalizer.transform(test_data_sequence)\n",
    "\n",
    "# --- ADD THIS SANITY CHECK ---\n",
    "print(f\"Normalized train data mean: {train_data.mean()}\")\n",
    "print(f\"Normalized train data std: {train_data.std()}\")\n",
    "# -----------------------------\n",
    "\n",
    "# Free up memory\n",
    "del train_data_sequence\n",
    "del test_data_sequence\n",
    "print(\"Normalization complete. Raw data cleared from RAM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Define Simple Dataset Class ---\n",
    "class TimeSteppingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple dataset that just returns the (t, t+1) pairs\n",
    "    from a pre-normalized data sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_sequence):\n",
    "        self.data = data_sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'x': self.data[idx], 'y': self.data[idx + 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Create Datasets and DataLoaders ---\n",
    "# Create the datasets from your NEW normalized tensors\n",
    "train_dataset = TimeSteppingDataset(train_data)\n",
    "test_dataset = TimeSteppingDataset(test_data)\n",
    "\n",
    "# Create the DataLoaders\n",
    "# Try a small batch size first due to memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 18887843 parameters.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Define Model, Optimizer, Loss ---\n",
    "model = FNO(\n",
    "    n_modes=(16, 16, 16),\n",
    "    hidden_channels=32,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    n_layers=4\n",
    ").to(device) \n",
    "\n",
    "print(f\"Model has {count_model_params(model)} parameters.\")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4) # Using the lower 1e-4 lr\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "l2loss = LpLoss(d=3, p=2)\n",
    "h1loss = H1Loss(d=3)\n",
    "\n",
    "# --- 8. Create Trainer (No Processor) ---\n",
    "trainer = Trainer(model=model, n_epochs=1000,\n",
    "                  device=device,\n",
    "                  wandb_log=False,\n",
    "                  eval_interval=10,\n",
    "                  use_distributed=False,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on full, normalized dataset...\n",
      "Training on 807 samples\n",
      "Testing on [201] samples         on resolutions [32].\n",
      "Raw outputs of shape torch.Size([4, 3, 32, 32, 32])\n",
      "[0] time=15.88, avg_loss=15059018.5891, train_err=60161524.7596\n",
      "Eval: 32_h1=1997268.5000, 32_l2=807935.3750\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[10] time=15.92, avg_loss=2591223.2009, train_err=10352064.9658\n",
      "Eval: 32_h1=3431261.5000, 32_l2=2669713.5000\n",
      "[20] time=15.92, avg_loss=1116687.0498, train_err=4461220.0454\n",
      "Eval: 32_h1=626017.3125, 32_l2=362783.4062\n",
      "[30] time=15.92, avg_loss=754442.1228, train_err=3014033.6292\n",
      "Eval: 32_h1=950500.2500, 32_l2=951149.7500\n",
      "[40] time=15.93, avg_loss=234416.5870, train_err=936505.8697\n",
      "Eval: 32_h1=266810.7500, 32_l2=158060.8906\n",
      "[50] time=15.92, avg_loss=135453.8060, train_err=541144.6605\n",
      "Eval: 32_h1=129346.8047, 32_l2=5864.8750\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[60] time=15.94, avg_loss=325914.5363, train_err=1302044.7070\n",
      "Eval: 32_h1=275220.3438, 32_l2=200938.5469\n",
      "[70] time=15.92, avg_loss=706020.0647, train_err=2820585.1101\n",
      "Eval: 32_h1=1793057.2500, 32_l2=1839085.7500\n",
      "[80] time=15.89, avg_loss=2142697.2223, train_err=8560181.4773\n",
      "Eval: 32_h1=3320112.2500, 32_l2=3494016.2500\n",
      "[90] time=15.90, avg_loss=3347327.6520, train_err=13372739.6790\n",
      "Eval: 32_h1=3756161.2500, 32_l2=3848482.2500\n",
      "[100] time=16.04, avg_loss=2496462.1176, train_err=9973489.7469\n",
      "Eval: 32_h1=859332.2500, 32_l2=310581.9688\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[110] time=15.92, avg_loss=1495910.2969, train_err=5976235.6912\n",
      "Eval: 32_h1=1087319.7500, 32_l2=993290.0000\n",
      "[120] time=15.90, avg_loss=775369.1071, train_err=3097637.9677\n",
      "Eval: 32_h1=1715239.8750, 32_l2=1243982.2500\n",
      "[130] time=15.94, avg_loss=562899.7294, train_err=2248812.2851\n",
      "Eval: 32_h1=1007719.5625, 32_l2=1092251.6250\n",
      "[140] time=15.95, avg_loss=241765.8092, train_err=965866.3762\n",
      "Eval: 32_h1=320066.7500, 32_l2=332214.1250\n",
      "[150] time=15.96, avg_loss=58661.0962, train_err=234353.9832\n",
      "Eval: 32_h1=62790.0938, 32_l2=2375.9343\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[160] time=15.92, avg_loss=219015.5013, train_err=874977.7701\n",
      "Eval: 32_h1=387904.7812, 32_l2=337031.9062\n",
      "[170] time=15.90, avg_loss=421284.1057, train_err=1683050.8581\n",
      "Eval: 32_h1=307338.1562, 32_l2=309429.3438\n",
      "[180] time=15.90, avg_loss=1229311.9287, train_err=4911162.0122\n",
      "Eval: 32_h1=794203.0625, 32_l2=742019.0000\n",
      "[190] time=15.92, avg_loss=3242864.9210, train_err=12955405.8973\n",
      "Eval: 32_h1=1266513.5000, 32_l2=987310.3750\n",
      "[200] time=15.88, avg_loss=1977131.5527, train_err=7898738.4310\n",
      "Eval: 32_h1=717322.8125, 32_l2=629750.9375\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[210] time=15.87, avg_loss=1226783.5164, train_err=4901060.8799\n",
      "Eval: 32_h1=1666889.6250, 32_l2=1535737.7500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Use the shape of one test sample as the key\u001b[39;00m\n\u001b[32m      4\u001b[39m test_key = test_data[\u001b[32m0\u001b[39m].shape[\u001b[32m1\u001b[39m] \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m              \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43mtest_key\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m              \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m              \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m              \u001b[49m\u001b[43mtraining_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mh1loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m              \u001b[49m\u001b[43meval_losses\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mh1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mh1loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ml2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2loss\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                \u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./checkpoints/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/neuralop/training/trainer.py:235\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_loader, test_loaders, optimizer, scheduler, regularizer, training_loss, eval_losses, eval_modes, save_every, save_best, save_dir, resume_from_dir, max_autoregressive_steps)\u001b[39m\n\u001b[32m    227\u001b[39m     sys.stdout.flush()\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.start_epoch, \u001b[38;5;28mself\u001b[39m.n_epochs):\n\u001b[32m    230\u001b[39m     (\n\u001b[32m    231\u001b[39m         train_err,\n\u001b[32m    232\u001b[39m         avg_loss,\n\u001b[32m    233\u001b[39m         avg_lasso_loss,\n\u001b[32m    234\u001b[39m         epoch_train_time,\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m     epoch_metrics = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    237\u001b[39m         train_err=train_err,\n\u001b[32m    238\u001b[39m         avg_loss=avg_loss,\n\u001b[32m    239\u001b[39m         avg_lasso_loss=avg_lasso_loss,\n\u001b[32m    240\u001b[39m         epoch_train_time=epoch_train_time,\n\u001b[32m    241\u001b[39m     )\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[38;5;28mself\u001b[39m.eval_interval == \u001b[32m0\u001b[39m:\n\u001b[32m    244\u001b[39m         \u001b[38;5;66;03m# evaluate and gather metrics across each loader in test_loaders\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch_env/lib/python3.11/site-packages/neuralop/training/trainer.py:301\u001b[39m, in \u001b[36mTrainer.train_one_epoch\u001b[39m\u001b[34m(self, epoch, train_loader, training_loss)\u001b[39m\n\u001b[32m    298\u001b[39m loss.backward()\n\u001b[32m    299\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m train_err += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    303\u001b[39m     avg_loss += loss.item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- 9. Start Training ---\n",
    "print(\"Starting training on full, normalized dataset...\")\n",
    "# Use the shape of one test sample as the key\n",
    "test_key = test_data[0].shape[1] \n",
    "trainer.train(train_loader=train_loader,\n",
    "              test_loaders={test_key: test_loader},\n",
    "              optimizer=optimizer,\n",
    "              scheduler=scheduler,\n",
    "              training_loss=h1loss,\n",
    "              eval_losses={'h1': h1loss, 'l2': l2loss},\n",
    "                save_every=50,\n",
    "                save_dir='./checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
