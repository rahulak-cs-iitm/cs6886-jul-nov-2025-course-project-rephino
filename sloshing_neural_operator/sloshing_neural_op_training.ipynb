{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy.io as sio\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from neuralop.models import FNO\n",
    "from neuralop import Trainer\n",
    "from neuralop.training import AdamW\n",
    "from neuralop import LpLoss, H1Loss\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop.data.transforms.normalizers import UnitGaussianNormalizer\n",
    "from abc import abstractmethod\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 data files.\n",
      "Loading training data into memory...\n",
      "Full training sequence shape: torch.Size([40400, 3, 32, 32, 32])\n",
      "Loading validation data into memory...\n",
      "Full validation sequence shape: torch.Size([20200, 3, 32, 32, 32])\n",
      "Loading test data into memory...\n",
      "Full test sequence shape: torch.Size([40400, 3, 32, 32, 32])\n",
      "Fitting normalizer on training data...\n",
      "Fit complete.\n",
      "Normalizing data...\n",
      "Normalized train data mean: 4.94738250367277e-10\n",
      "Normalized train data std: 0.9999978542327881\n",
      "Normalization complete. Raw data cleared from RAM.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Find and Load All Data Files ---\n",
    "data_path = './FNO_Dataset_PT/'\n",
    "file_paths = glob.glob(f\"{data_path}/FNO_dataset_run_*.pt\")\n",
    "file_paths.sort()\n",
    "\n",
    "if not file_paths:\n",
    "    raise FileNotFoundError(f\"No .mat files found in {data_path}\")\n",
    "\n",
    "print(f\"Found {len(file_paths)} data files.\")\n",
    "\n",
    "# --- 2. Split File Paths into Train and Test ---\n",
    "train_fraction = 0.4\n",
    "validation_fraction = 0.2\n",
    "\n",
    "train_split = int(train_fraction * len(file_paths))\n",
    "valid_split = int((train_fraction + validation_fraction) * len(file_paths))\n",
    "train_paths = file_paths[:train_split]\n",
    "valid_paths = file_paths[train_split:valid_split]\n",
    "test_paths = file_paths[valid_split:]\n",
    "\n",
    "# --- 3. Load ALL Data into RAM for Normalization ---\n",
    "# This is the workflow you prefer. It requires\n",
    "# loading all training data into memory first.\n",
    "\n",
    "def load_data_from_paths(paths): # <-- Removed data_key\n",
    "    all_tensors = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            # Load the .pt file directly as a tensor\n",
    "            tensor_data = torch.load(p).float() # <-- Changed loading function\n",
    "            all_tensors.append(tensor_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error loading {p}: {e}\")\n",
    "    # Concatenate all runs along the time dimension (dim=0)\n",
    "    return torch.cat(all_tensors, dim=0)\n",
    "\n",
    "print(\"Loading training data into memory...\")\n",
    "train_data_sequence = load_data_from_paths(train_paths)\n",
    "print(f\"Full training sequence shape: {train_data_sequence.shape}\")\n",
    "\n",
    "print(\"Loading validation data into memory...\")\n",
    "validation_data_sequence = load_data_from_paths(valid_paths)\n",
    "print(f\"Full validation sequence shape: {validation_data_sequence.shape}\")\n",
    "\n",
    "print(\"Loading test data into memory...\")\n",
    "test_data_sequence = load_data_from_paths(test_paths)\n",
    "print(f\"Full test sequence shape: {test_data_sequence.shape}\")\n",
    "\n",
    "# --- 4. Fit and Transform (Your Method) ---\n",
    "# Create the normalizer\n",
    "normalizer = UnitGaussianNormalizer(dim=[0, 2, 3, 4]) \n",
    "\n",
    "# Fit ONLY on the training data\n",
    "print(\"Fitting normalizer on training data...\")\n",
    "normalizer.fit(train_data_sequence)\n",
    "print(\"Fit complete.\")\n",
    "\n",
    "# Transform both sets\n",
    "print(\"Normalizing data...\")\n",
    "train_data = normalizer.transform(train_data_sequence)\n",
    "valid_data = normalizer.transform(validation_data_sequence)\n",
    "test_data = normalizer.transform(test_data_sequence)\n",
    "\n",
    "# --- ADD THIS SANITY CHECK ---\n",
    "print(f\"Normalized train data mean: {train_data.mean()}\")\n",
    "print(f\"Normalized train data std: {train_data.std()}\")\n",
    "# -----------------------------\n",
    "\n",
    "# Free up memory\n",
    "del train_data_sequence\n",
    "del validation_data_sequence\n",
    "del test_data_sequence\n",
    "print(\"Normalization complete. Raw data cleared from RAM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Define Simple Dataset Class ---\n",
    "class TimeSteppingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple dataset that just returns the (t, t+1) pairs\n",
    "    from a pre-normalized data sequence.\n",
    "    \n",
    "    This version is \"run-aware\" to prevent mixing\n",
    "    data from different simulation runs.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_sequence, steps_per_run):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_sequence (torch.Tensor): The giant tensor of all runs\n",
    "            steps_per_run (int): The number of time steps in EACH run\n",
    "                                 (e.g., 100)\n",
    "        \"\"\"\n",
    "        self.data = data_sequence\n",
    "        self.steps_per_run = steps_per_run\n",
    "        \n",
    "        # We can't use the last step of *any* run as an input 'x'\n",
    "        self.valid_pairs_per_run = self.steps_per_run - 1\n",
    "        \n",
    "        # Calculate how many runs are in this tensor\n",
    "        self.num_runs = self.data.shape[0] // self.steps_per_run\n",
    "        \n",
    "        if self.data.shape[0] % self.steps_per_run != 0:\n",
    "            print(f\"Warning: Data shape {self.data.shape[0]} is not \"\n",
    "                  f\"perfectly divisible by steps_per_run {self.steps_per_run}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of *valid* (t, t+1) pairs.\n",
    "        \"\"\"\n",
    "        return self.num_runs * self.valid_pairs_per_run\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Gets the N-th *valid* pair, skipping boundaries.\n",
    "        'idx' will be from 0 to (total_valid_pairs - 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Which run is this pair in?\n",
    "        # e.g., if valid_pairs_per_run=99, idx=100 -> run_index=1\n",
    "        run_index = idx // self.valid_pairs_per_run\n",
    "        \n",
    "        # 2. What is the time-index *within* that run?\n",
    "        # e.g., if valid_pairs_per_run=99, idx=100 -> time_index=1\n",
    "        time_index_in_run = idx % self.valid_pairs_per_run\n",
    "        \n",
    "        # 3. What is the *actual* index in the giant data tensor?\n",
    "        # This calculation skips the boundary indices.\n",
    "        # e.g., run_index=1, time_index=1 -> (1 * 100) + 1 = 101\n",
    "        global_start_idx = (run_index * self.steps_per_run) + time_index_in_run\n",
    "        \n",
    "        # This will now correctly get (e.g.) data[101] and data[102]\n",
    "        # and will *never* ask for (data[99], data[100])\n",
    "        x = self.data[global_start_idx]\n",
    "        y = self.data[global_start_idx + 1]\n",
    "        \n",
    "        return {'x': x, 'y': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Create Datasets and DataLoaders ---\n",
    "\n",
    "# You must know this value from your data generation\n",
    "# For example, if each .pt file had 100 time steps:\n",
    "STEPS_PER_RUN = 101 \n",
    "\n",
    "# Create the datasets from your NEW normalized tensors\n",
    "train_dataset = TimeSteppingDataset(train_data, steps_per_run=STEPS_PER_RUN)\n",
    "valid_dataset = TimeSteppingDataset(valid_data, steps_per_run=STEPS_PER_RUN)\n",
    "test_dataset = TimeSteppingDataset(test_data, steps_per_run=STEPS_PER_RUN)\n",
    "\n",
    "# Create the DataLoaders\n",
    "# Try a small batch size first due to memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=100, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 20731 parameters.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Define Model, Optimizer, Loss ---\n",
    "model = FNO(\n",
    "    n_modes=(8, 8, 8),\n",
    "    hidden_channels=4,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    n_layers=2\n",
    ").to(device) \n",
    "\n",
    "print(f\"Model has {count_model_params(model)} parameters.\")\n",
    "\n",
    "n_epochs = 1000\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4) # Using the lower 1e-4 lr\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "l2loss = LpLoss(d=3, p=2)\n",
    "h1loss = H1Loss(d=3)\n",
    "\n",
    "# --- 8. Create Trainer (No Processor) ---\n",
    "trainer = Trainer(model=model, n_epochs=n_epochs,\n",
    "                  device=device,\n",
    "                  wandb_log=False,\n",
    "                  eval_interval=1,\n",
    "                  use_distributed=False,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on full, normalized dataset...\n",
      "Training on 40000 samples\n",
      "Testing on [20000] samples         on resolutions [32].\n",
      "Raw outputs of shape torch.Size([100, 3, 32, 32, 32])\n",
      "[0] time=35.98, avg_loss=987472712.7040, train_err=98747271270.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prad/personal/CS6886/cs6886-jul-nov-2025-course-project-rephino/sloshing_neural_operator/neuraloperator/neuralop/training/trainer.py:581: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.\n",
      "  val_loss = loss(out, **sample)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: 32_h1=628448064.0000, 32_l2=571243904.0000\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[1] time=38.21, avg_loss=163262724.1744, train_err=16326272417.4400\n",
      "Eval: 32_h1=165552496.0000, 32_l2=12649311.0000\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[2] time=38.49, avg_loss=7835693.8028, train_err=783569380.2800\n",
      "Eval: 32_h1=63340436.0000, 32_l2=4542016.5000\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[3] time=38.19, avg_loss=3449858.7892, train_err=344985878.9200\n",
      "Eval: 32_h1=40699080.0000, 32_l2=2728103.0000\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[4] time=39.03, avg_loss=2235241.8364, train_err=223524183.6400\n",
      "Eval: 32_h1=31251836.0000, 32_l2=1822518.8750\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[5] time=39.03, avg_loss=1518093.1110, train_err=151809311.1000\n",
      "Eval: 32_h1=24791632.0000, 32_l2=1255793.0000\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[6] time=39.01, avg_loss=1081270.4940, train_err=108127049.4000\n",
      "Eval: 32_h1=20075232.0000, 32_l2=907543.5000\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[7] time=39.67, avg_loss=829370.7045, train_err=82937070.4500\n",
      "Eval: 32_h1=16280587.0000, 32_l2=702981.6250\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[8] time=38.29, avg_loss=704006.7725, train_err=70400677.2500\n",
      "Eval: 32_h1=13343480.0000, 32_l2=598999.0000\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[9] time=39.08, avg_loss=543534.6665, train_err=54353466.6550\n",
      "Eval: 32_h1=11095621.0000, 32_l2=1236535.6250\n",
      "[10] time=39.04, avg_loss=596391.5082, train_err=59639150.8250\n",
      "Eval: 32_h1=9374136.0000, 32_l2=374645.3125\n",
      "[Rank 0]: saved training state to ./checkpoints/\n",
      "[11] time=40.05, avg_loss=396481.1496, train_err=39648114.9600\n",
      "Eval: 32_h1=7934494.0000, 32_l2=436027.0312\n",
      "[12] time=42.05, avg_loss=405850.0372, train_err=40585003.7250\n",
      "Eval: 32_h1=6902338.0000, 32_l2=575250.0625\n",
      "[13] time=38.84, avg_loss=478824.6733, train_err=47882467.3350\n",
      "Eval: 32_h1=6145885.0000, 32_l2=296928.9062\n",
      "[Rank 0]: saved training state to ./checkpoints/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Use the shape of one test sample as the key\u001b[39;00m\n\u001b[32m      4\u001b[39m valid_key = valid_data[\u001b[32m0\u001b[39m].shape[\u001b[32m1\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m              \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43mvalid_key\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m              \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m              \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m              \u001b[49m\u001b[43mtraining_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43ml2loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m              \u001b[49m\u001b[43meval_losses\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mh1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mh1loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ml2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2loss\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                \u001b[49m\u001b[43msave_best\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvalid_key\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_l2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./checkpoints/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal/CS6886/cs6886-jul-nov-2025-course-project-rephino/sloshing_neural_operator/neuraloperator/neuralop/training/trainer.py:235\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_loader, test_loaders, optimizer, scheduler, regularizer, training_loss, eval_losses, eval_modes, save_every, save_best, save_dir, resume_from_dir, max_autoregressive_steps)\u001b[39m\n\u001b[32m    227\u001b[39m     sys.stdout.flush()\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.start_epoch, \u001b[38;5;28mself\u001b[39m.n_epochs):\n\u001b[32m    230\u001b[39m     (\n\u001b[32m    231\u001b[39m         train_err,\n\u001b[32m    232\u001b[39m         avg_loss,\n\u001b[32m    233\u001b[39m         avg_lasso_loss,\n\u001b[32m    234\u001b[39m         epoch_train_time,\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m     epoch_metrics = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    237\u001b[39m         train_err=train_err,\n\u001b[32m    238\u001b[39m         avg_loss=avg_loss,\n\u001b[32m    239\u001b[39m         avg_lasso_loss=avg_lasso_loss,\n\u001b[32m    240\u001b[39m         epoch_train_time=epoch_train_time,\n\u001b[32m    241\u001b[39m     )\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[38;5;28mself\u001b[39m.eval_interval == \u001b[32m0\u001b[39m:\n\u001b[32m    244\u001b[39m         \u001b[38;5;66;03m# evaluate and gather metrics across each loader in test_loaders\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal/CS6886/cs6886-jul-nov-2025-course-project-rephino/sloshing_neural_operator/neuraloperator/neuralop/training/trainer.py:296\u001b[39m, in \u001b[36mTrainer.train_one_epoch\u001b[39m\u001b[34m(self, epoch, train_loader, training_loss)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;66;03m# track number of training examples in batch\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[38;5;28mself\u001b[39m.n_samples = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:171\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections.abc.MutableMapping):\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[32m    169\u001b[39m     clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m     clone.update(\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[43m{\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:172\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections.abc.MutableMapping):\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[32m    169\u001b[39m     clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m     clone.update(\n\u001b[32m    171\u001b[39m         {\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[32m    176\u001b[39m         }\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fno_env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- 9. Start Training ---\n",
    "print(\"Starting training on full, normalized dataset...\")\n",
    "# Use the shape of one test sample as the key\n",
    "valid_key = valid_data[0].shape[1]\n",
    "trainer.train(train_loader=train_loader,\n",
    "              test_loaders={valid_key: valid_loader},\n",
    "              optimizer=optimizer,\n",
    "              scheduler=scheduler,\n",
    "              training_loss=l2loss,\n",
    "              eval_losses={'h1': h1loss, 'l2': l2loss},\n",
    "                save_best=f'{valid_key}_l2',\n",
    "                save_dir='./checkpoints/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fno_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
